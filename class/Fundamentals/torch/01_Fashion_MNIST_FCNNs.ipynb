{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fashion MNIST with PyTorch",
   "id": "2ef824c57a5d52f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/PabloRR100/intro_deep_learning/blob/main/class/Fundamentals/torch/01_Fashion_MNIST_FCNNs.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/PabloRR100/intro_deep_learning/blob/main/class/Fundamentals/torch/01_Fashion_MNIST_FCNNs.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ],
   "id": "73c188a48eaaad01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fashion MNIST with PyTorch",
   "id": "8258d5327e67206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fashion MNIST with PyTorch",
   "id": "b41414567973d0af"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eaa479cfdc095e2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "be894f93e260b086"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # where to download data to?\n",
    "    train=True,  # get training data\n",
    "    download=True,  # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(),  # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None  # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,  # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ],
   "id": "7a533a5cb0f7c8c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# What's the shape of the image?# See first training sample\n",
    "image, label = train_data[0]"
   ],
   "id": "222fe6a3010da554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# image, label",
   "id": "1a4fd9b371eb7f53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "image.shape\n",
   "id": "ac3153f5d493d8fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# How many samples are there? \n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ],
   "id": "d578aec18399f8d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# See classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ],
   "id": "4b507168f5b2d4d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualize some training data\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())  # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ],
   "id": "c5426991f0f17de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ],
   "id": "28d6ca69a7999e5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:17.982655Z",
     "start_time": "2025-04-24T10:04:17.981199Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c4ed9a903a15e633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare a DataLoader",
   "id": "57e2016bd217c7c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:18.927469Z",
     "start_time": "2025-04-24T10:04:18.925616Z"
    }
   },
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader",
   "id": "abd64eb4243a7d1d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:19.348325Z",
     "start_time": "2025-04-24T10:04:19.346179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,  # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE,  # how many samples per batch? \n",
    "    shuffle=True  # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ],
   "id": "4b78db9c63193926",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x13d1b6d90>, <torch.utils.data.dataloader.DataLoader object at 0x13d196a10>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:19.902392Z",
     "start_time": "2025-04-24T10:04:19.897731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ],
   "id": "a7308c08f06d5516",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:20.448666Z",
     "start_time": "2025-04-24T10:04:20.423992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check out what's inside the testing dataloader\n",
    "# Show a sample\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ],
   "id": "84a7662cb966c58d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 5, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEk1JREFUeJzt3XtslXf9wPHvoaUt1EFhYVsFxly9EF0MymY2jZfpZNPJlKhRMwMzumE0TqdmDjX+4SUxY/ESjTOaqGFe4mXzskWneCFGhW3ROCdmKhN0E7BAoJRbS9vzy3OSfTZ+8Put36/24Qxfr4QMuufT53Ao593nnPZDo9lsNhMApJSmnewbAED7EAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAXI8JWvfCU1Go20bdu27NmrrroqnXPOOVNyu+A/RRRoe/fdd196zWtekxYtWpR6enrS/Pnz00tf+tL0mc985mTfNDjliAJt7Te/+U06//zz07333puuvvrq9NnPfja95S1vSdOmTUuf/vSnT/bNg1NO58m+AfD/+djHPpZmz56d7rnnntTX13fM/xscHDxptwtOVa4UaGsPPPBAesYznnFcECpnnHFG/PzLX/5yevGLX9x6W3d3d3r605+ebr755uNmquf0X/GKV6Rf/epX6TnPeU7r6ahzzz03rVu37rhjN2/e3HqfM2bMSAsWLEgf/ehH08TExHHHff/730+XX355euITn9g698DAQPrIRz6SxsfH/yP3AdTJlQJtrXodYePGjemPf/xjOu+88/7P46oAVPG44oorUmdnZ7r99tvT2972ttaD+Nvf/vZjjt2yZUvrNYo3v/nNadWqVelLX/pS60XgpUuXtt5HZefOneniiy9OY2Nj6YYbbki9vb3pC1/4QisQJ3rx+QlPeEJ697vf3frvz3/+8/ShD30o7d+/P61du3YK7hWYQtW/pwDt6ic/+Umzo6Oj9eOiiy5qXn/99c0f//jHzdHR0WOOO3To0HGzl156afPcc8895m2LFi2q/v2Q5i9/+ct42+DgYLO7u7v5nve8J972rne9q3XcXXfddcxxs2fPbr1969at/++5V69e3Zw5c2bzyJEj8bZVq1a1zg/tzNNHtLXqq4yqK4XqCqB6sfnGG29Ml156aesrkH7wgx/EcY/+DH5oaCjt3r07vfCFL0x/+9vfWr9+tOqppec///nx63nz5qWnPe1prWMf9sMf/jBdeOGFraeYHn3clVdeedxtfPS5h4eHW+eu3v+hQ4fS/fff/x+6J6AeokDbu+CCC9Jtt92W9u7dm+6+++60Zs2a1oNv9RTQn/70p9Yxv/71r9Mll1zSepqnev2hegB///vf3/p//zsKZ5999nHnmDNnTuv9P+zvf/97espTnnLccVU8TvTaw4oVK1oviM+aNat17je+8Y0nPDe0O68p8LjR1dXVCkT146lPfWp605velL797W+3HoBf8pKXpMWLF6dPfOITaeHCha1jq8/2P/nJTx734nBHR8cJ33/Jv0y7b9++1hVJFYMPf/jDrReZqxevf/e736X3ve99J3xhGtqZKPC4VH3vQmXHjh2tF5VHRkZaTyc9+irgF7/4xb/1Avdf//rX497+5z//+Zhfb9iwIe3Zs6d1JfOCF7wg3r5169bic8PJ5Okj2lr1wH6iz+Crq4CHn855+DP/Rx9XPW1TfZlqqZe//OVp06ZNraerHrZr1670ta997ZjjTnTu0dHR9LnPfa743HAyuVKgrb3jHe9ovWBbPWdfPT1UPeBW3+X8zW9+s/U9B9VTSP/6179aTxctX748rV69Oh04cCB98YtfbH3PQnUlUeL6669Pt9xyS7rsssvSO9/5zviS1OoK4g9/+EMc99znPrf1ekT1pa3XXnttay9SNVfyVBS0A1cKtLWbbrqp9f0C1ZVB9X0A1Y/qs/fqexDuuuuu1ovK1dXCd77zndYD8nvf+970+c9/Pl1zzTWtB/NS/f39rauUZz7zmenjH/94+tSnPpVWrlx53Ps8/fTT0x133NE6/oMf/GDr9lZfMVV9lRQ8HjWqr0s92TcCgPbgSgGAIAoABFEAIIgCAEEUAAiiAED+N69VXwMOwOPXZL4DwZUCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBC5yM/hVNHd3d39szY2Fj2zPj4eKrL9OnTs2eOHj2a2lWj0SiaazabqV0tWbKkaO6iiy7Knrn55pvTVHClAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAYCEep6SRkZG2XbxXettKFva1s9LFdv39/dkzixYtyp6ZN29e9swVV1yRSjz00EPZMzNmzEhTwZUCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBCoznJrVSNRmMyh3GSlfw5lS4mq0vJ0rmSxWS33XZb9sz4+HiqS0dHR/bMxMRE2348LFmypGju1a9+dfbMrFmzsmf27duXPbNp06ZUYsOGDdkzhw8fnpI/W1cKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAInY/8lP9WJUv0pk2bVtvyuBUrVmTPLFu2LHvmwQcfrG0BWok6l+/lOu+887JnVq9eXXSu7du3Z89s27Yte+a+++7LnvnpT3+aHu9cKQAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAMGW1FNMs9nMnuno6GjrjZ2XX3559szKlStrue8WL16cPXPZZZelEl1dXdkza9euzZ553vOelz3zqle9KntmeHg4lZiYmMieuf3227NntmzZkurSTn8HXSkAEEQBgCAKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACA0mpPcAtZoNCZzGPzHLV26NHtmwYIF2TObN2/OnjnrrLOyZ773ve+lEnv37s2eufPOO7Nnbr311uyZOXPmZM9897vfzZ7h3zOZh3tXCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACBbiUZv+/v6iuR07dqQ6nHPOOdkz27Zty5655557Uon169dnz9x4443ZM319fbXcD+2uu7s7e2bJkiVF51q0aFH2zN133509s3Xr1sc8xpUCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBC5yM/5VRQsrhwkjsRj3HxxRdnzxw+fDiV6Orqyp75xz/+0bZL3S644IKiude+9rXZM2NjY7XcD1dffXX2TE9PTypxyy231LLscPny5bXc36Uf44ODg2kquFIAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBCoznJFZnTptXTj5KNne2+UbTdLV68OHvm4MGD2TPXXXddKrF58+bsmQ0bNmTP7Ny5s5b7oU4DAwPZMwsXLsyeeetb35o987rXvS6VuPfee7Nnbr311uyZoaGh7JmRkZFUYmJiIntm/fr12TNbt259zGNcKQAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIH8hXsnyuFNRd3d3LecpXaw1f/787Jnh4eHsmf3792fPdHR0pBLLli3Lnpk9e3YtS8keeOCB7Jnf/va3qS5r1qzJnnn2s5+dPbN9+/bsmXnz5qUSGzduzJ7ZtWtX9kxvb28tM5V9+/Zlz/zsZz/LnnnooYce8xhXCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACJ2P/JSpXFSXa+7cuUVzAwMD2TMzZ87Mnlm/fn32zPj4eCrxox/9KNXh/PPPz55Zu3ZtLYvMKnfeeWctiwsffPDB7JmxsbHa7oeSxYX9/f3ZM4cOHartY7yvry975uDBg2kquFIAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEBoNJvNZpqERqMxmcNOedddd132zMte9rLsmWuvvTaVuP/++7NnVqxYkT3T29ubPfONb3wjlShdMlaHs846K3tm5cqVRecaHh5OdShZtFaypG7OnDmpRMnH3tDQUC2/p4mCmVJf//rXs2cOHDjwmMe4UgAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQOhM/8Xmz5+fPXPmmWdmzyxfvry2ZWElNm3alD3zohe9KHvmyiuvTCXWrVuX2tXrX//67JlLLrmk6Fx33HFH9syuXbuyZ/r6+rJnDh8+nD1z6NChVGJ0dDR7prMz/6Guu7u7tqWFJbdvqrhSACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAUAQqPZbDbTJDQajckcdsrr7+/PntmxY0f2TOn9Pck/zpOyYfaaa64pOlfJ5smbbrope2bp0qXZM8uWLcuemTat7HOxv/zlL9kzs2bNyp7p6urKntm3b1/2zPTp01OJ0047LdVhbGyslg2upfd5yfbgyWymdaUAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYDQmSZpYGAg5XrSk56UPbNz585UomQRVcnMzJkzs2f279+fPfOBD3wglfjWt76VPfP73/8+e+af//xn9szGjRtTiRUrVmTPvOENb8ieWbhwYfbM4ODglCwlO5G5c+emdtXd3Z09MzIyUnSukvuvp6cn1WFa4bLDkuV7R44cSVPBlQIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAPIX4pUsvFqwYEH2zOzZs1OJrq6u7Jnp06dnz+zZsyd75pWvfGX2zFVXXZVKrFq1KntmzZo12TO7du3Knlm8eHEqUbJ0ruTjdfv27dkz4+PjtSxVrAwNDWXP9Pb21rLUreTvUskSuMrRo0ezZzo7J/1QFyYmJlIdM5Vms1nLzGS4UgAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQJj0lqjdu3enXFu2bKltWdjcuXNrWV51xhlnZM8cOHAge+aGG25IJUoWwZ1++unZMx0dHbXcD6WL00pmShbBlZyndGnajBkzalnYV6Knp6etF+KVnqsupR8TU8GVAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAUA8hfilTjzzDOzZ3bu3JnqUrJEr2Rp2t69e1NdhoeHs2dGRkZqWc5Wct9VhoaGUh1KlvyV3A+lms1mLeepazFgqd7e3lo+xhuNRvZMV1dXKrF///7ULlwpABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIA+VtSBwcHUx0bO5csWZJKHDlypJbNiaOjo9kzR48erWVjZ+kGybqUbkktmSu5z0t0dk7pouGT8vFasulz+vTpqS6HDx+u5fFhWsHHXekm25I/p6niSgGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAKHRnOQGp0ajkeowY8aMorkLL7wwe+bJT35y9sycOXNqWay1Z8+eVNfStJLlcePj49kzPT09qc5FenUsdatzIV7J/VDyeyr5u15yntI/15KP8QMHDtRy+2YUPn7t3r07e+arX/1q9sxkHu5dKQAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAoH0X4rW7vr6+7JmBgYHsmbPPPjuVmDt3bvZMV1dXLQvQSs5TGR0dzZ45ePBg9szExEQtywRLfj+Vw4cPZ8/s3bs3e2aSDwn/tpLFdpVnPetZtSyqGym4faeddloqMTg4mD2zbt267BkL8QDIIgoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAMFCPID/Ek0L8QDIIQoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBCZ5qkZrM52UMBeJxypQBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAetj/AOm9GjTuKfFlAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create the Models",
   "id": "c30bbd2829dbb4d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:04:22.095598Z",
     "start_time": "2025-04-24T10:04:22.093056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten()  # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x)  # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
    "\n",
    "# Try uncommenting below and see what happens\n",
    "#print(x)\n",
    "#print(output)"
   ],
   "id": "aa8e0ee819385dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:50.804599Z",
     "start_time": "2025-04-24T11:44:50.801327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that is only a sequence of Linear layers. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ],
   "id": "4b0a0a52df5e77ae",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:51.038873Z",
     "start_time": "2025-04-24T11:44:51.036865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Automatically detect device (cpu, gpu or mps)\n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Fix to CPU for the moment: \n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "ad018372c548fd05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:53.041049Z",
     "start_time": "2025-04-24T11:44:53.037472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to set up model with input parameters\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=784,  # one for every pixel (28x28)\n",
    "    hidden_units=10,  # how many units in the hidden layer\n",
    "    output_shape=len(class_names)  # one for every class\n",
    ")"
   ],
   "id": "399dc8e376ddbcc4",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:55.137966Z",
     "start_time": "2025-04-24T11:44:55.134935Z"
    }
   },
   "cell_type": "code",
   "source": "model_0.to(DEVICE)  # keep model on CPU to begin with",
   "id": "57ead90930d46972",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:55.971243Z",
     "start_time": "2025-04-24T11:44:55.968765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ],
   "id": "9aa3222fb626925e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:56.323227Z",
     "start_time": "2025-04-24T11:44:56.321238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy between predictions and true labels.\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()  # sum up all the correct predictions\n",
    "    acc = (correct / len(y_pred)) * 100  # divide by number of predictions\n",
    "    return acc"
   ],
   "id": "66dd9b43eba64a2f",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute initial accuracy -> How much do we expect? ",
   "id": "7d906b4be6244805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:56.787765Z",
     "start_time": "2025-04-24T11:44:56.644976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy_in_full_subset(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over an entire subset\n",
    "    \"\"\"\"\"\n",
    "    # P\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            # 1. Forward pass\n",
    "            preds = model(X)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=preds.argmax(dim=1))\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        acc /= len(test_dataloader)\n",
    "    return acc\n",
    "\n",
    "\n",
    "test_acc = compute_accuracy_in_full_subset(model=model_0, dataloader=test_dataloader)\n",
    "print(f\"Initial Test Accuracy: {test_acc}\")"
   ],
   "id": "60c25194eec18c61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Accuracy: 10.852635782747603\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:57.052311Z",
     "start_time": "2025-04-24T11:44:57.050669Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2dfa4abf954911da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:45:01.416611Z",
     "start_time": "2025-04-24T11:44:57.646032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss  # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "\n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y)  # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n"
   ],
   "id": "5a35edbb2a67acbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9abe37549734a1c84b3ea1b59e5ca7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create better models with Non-Linearities",
   "id": "ff3d0caa51557d2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T10:05:14.041355Z",
     "start_time": "2025-04-24T10:05:14.039257Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 26,
   "source": [
    "# Create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten inputs into single vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)\n"
   ],
   "id": "def50566fdcec269"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:47:19.480359Z",
     "start_time": "2025-04-24T11:47:19.478645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can move the device to \"mps\", \"gpu\" or \"cpu\"\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ],
   "id": "a60df8d8276960f0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:47:19.816398Z",
     "start_time": "2025-04-24T11:47:19.814583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since we are going to be creating more models, we'll create a function to help us do that\n",
    "from helpers import eval_model"
   ],
   "id": "7d7fe6f05b3e81db",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:47:22.637773Z",
     "start_time": "2025-04-24T11:47:22.629530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_1 = FashionMNISTModelV1(\n",
    "    input_shape=784,  # number of input features\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)  # number of output classes desired\n",
    ").to(DEVICE)  # send model to GPU if it's available\n",
    "\n",
    "next(model_1.parameters()).device  # check model device"
   ],
   "id": "9e692349daa0c687",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:47:22.932834Z",
     "start_time": "2025-04-24T11:47:22.930362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = ...\n",
    "optimizer = ..."
   ],
   "id": "43a1f40ebe80c9fc",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:47:27.148955Z",
     "start_time": "2025-04-24T11:47:27.144444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    accuracy_fn,\n",
    "    device: torch.device = \"cpu\"\n",
    "):\n",
    "    # Complete\n",
    "    ...\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    accuracy_fn,\n",
    "    device: torch.device = \"cpu\"\n",
    "):\n",
    "    # Complete\n",
    "    ..."
   ],
   "id": "f744429e68e0f451",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:07:14.860340Z",
     "start_time": "2025-04-24T12:07:10.705801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=model_1,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(\n",
    "        data_loader=test_dataloader,\n",
    "        model=model_1,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )"
   ],
   "id": "1c6f439bbf1d3b25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87d440d895a64d02be8a3e99f3b1eccd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.83143 | Train accuracy: 68.60%\n",
      "Test loss: 0.88264 | Test accuracy: 67.09%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.82675 | Train accuracy: 68.71%\n",
      "Test loss: 0.87636 | Test accuracy: 67.09%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.82094 | Train accuracy: 68.98%\n",
      "Test loss: 0.86035 | Test accuracy: 67.95%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:07:15.014151Z",
     "start_time": "2025-04-24T12:07:14.865156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: This will error due to `eval_model()` not using device agnostic code \n",
    "model_1_results = eval_model(\n",
    "    model=model_1,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")"
   ],
   "id": "b627705614c59147",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:07:20.134944Z",
     "start_time": "2025-04-24T12:07:20.132001Z"
    }
   },
   "cell_type": "code",
   "source": "model_1_results",
   "id": "cdf72c47915574a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV1',\n",
       " 'model_loss': 0.8603453040122986,\n",
       " 'model_acc': 67.95127795527156}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:07:20.967968Z",
     "start_time": "2025-04-24T12:07:20.818237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check baseline results# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(\n",
    "    model=model_0,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")"
   ],
   "id": "db04a588b14eac13",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:07:22.369724Z",
     "start_time": "2025-04-24T12:07:22.367416Z"
    }
   },
   "cell_type": "code",
   "source": "model_0_results",
   "id": "98ee4602ea1dc676",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47663894295692444,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It looks like we are getting worse results !  \n",
    "What is happening here ?"
   ],
   "id": "368f7bcd9b5c5167"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prevent Overfitting",
   "id": "be2b975b578356f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:08:56.312382Z",
     "start_time": "2025-04-24T12:08:56.309986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten inputs into single vector\n",
    "            nn.BatchNorm1d(num_features=input_shape),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ],
   "id": "8225798747d541c1",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:08:56.900740Z",
     "start_time": "2025-04-24T12:08:56.891416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_2 = FashionMNISTModelV2(\n",
    "    input_shape=784,  # number of input features\n",
    "    hidden_units=50,\n",
    "    output_shape=len(class_names)  # number of output classes desired\n",
    ").to(DEVICE)  #"
   ],
   "id": "1f2396ced7bf5a65",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:09:05.611061Z",
     "start_time": "2025-04-24T12:08:58.160727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(\n",
    "        data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )"
   ],
   "id": "76b879494033692a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9da11f5a1664cebb83f881c75aad74e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 2.40255 | Train accuracy: 9.57%\n",
      "Test loss: 2.42558 | Test accuracy: 9.87%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 2.42458 | Train accuracy: 9.84%\n",
      "Test loss: 2.42558 | Test accuracy: 9.87%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 2.42458 | Train accuracy: 9.84%\n",
      "Test loss: 2.42558 | Test accuracy: 9.87%\n",
      "\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 2.42458 | Train accuracy: 9.84%\n",
      "Test loss: 2.42558 | Test accuracy: 9.87%\n",
      "\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 2.42458 | Train accuracy: 9.84%\n",
      "Test loss: 2.42558 | Test accuracy: 9.87%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Why is the model not improving ????",
   "id": "dbad1606d29390fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:12:19.425138Z",
     "start_time": "2025-04-24T12:12:19.422352Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer_2 = torch.optim.SGD(params=model_2.parameters(), lr=0.1)",
   "id": "fe4b7491a46e04aa",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:12:49.428095Z",
     "start_time": "2025-04-24T12:12:42.312686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer_2,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(\n",
    "        data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )"
   ],
   "id": "bc3dc70200190ec3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ddc9d61a4b64d41aa1e896a3248c4ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.52026 | Train accuracy: 82.15%\n",
      "Test loss: 0.43368 | Test accuracy: 84.73%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.35101 | Train accuracy: 87.13%\n",
      "Test loss: 0.38381 | Test accuracy: 86.30%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.31225 | Train accuracy: 88.53%\n",
      "Test loss: 0.37009 | Test accuracy: 86.48%\n",
      "\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.29098 | Train accuracy: 89.21%\n",
      "Test loss: 0.35991 | Test accuracy: 87.01%\n",
      "\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.27468 | Train accuracy: 89.75%\n",
      "Test loss: 0.37265 | Test accuracy: 86.83%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:13:38.301353Z",
     "start_time": "2025-04-24T12:13:38.130979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_2_results"
   ],
   "id": "947a868128556149",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV2',\n",
       " 'model_loss': 0.3726479113101959,\n",
       " 'model_acc': 86.83107028753993}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare Models",
   "id": "8e4c4d7d375dfeb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:15:34.899191Z",
     "start_time": "2025-04-24T12:15:34.713622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
    "compare_results"
   ],
   "id": "34519a619ee709b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            model_name  model_loss  model_acc\n",
       "0  FashionMNISTModelV0    0.476639  83.426518\n",
       "1  FashionMNISTModelV1    0.860345  67.951278\n",
       "2  FashionMNISTModelV2    0.372648  86.831070"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_loss</th>\n",
       "      <th>model_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FashionMNISTModelV0</td>\n",
       "      <td>0.476639</td>\n",
       "      <td>83.426518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FashionMNISTModelV1</td>\n",
       "      <td>0.860345</td>\n",
       "      <td>67.951278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FashionMNISTModelV2</td>\n",
       "      <td>0.372648</td>\n",
       "      <td>86.831070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:15:54.640674Z",
     "start_time": "2025-04-24T12:15:54.592113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"accuracy (%)\")\n",
    "plt.ylabel(\"model\")"
   ],
   "id": "711309b8aff31985",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'model')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a Confusion Matrix",
   "id": "6a06fdc80a2fbd7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Make predictions with trained model\n",
    "y_preds = []\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "    # Send data and targets to target device\n",
    "    X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "    # Do the forward pass\n",
    "    y_logit = model_2(X)\n",
    "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
    "    # Put predictions on CPU for evaluation\n",
    "    y_preds.append(y_pred.cpu())\n",
    "# Concatenate list of predictions into a tensor\n",
    "y_pred_tensor = torch.cat(y_preds)\n"
   ],
   "id": "be99fea713fee248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:22:07.901829Z",
     "start_time": "2025-04-24T12:22:04.455396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# See if torchmetrics exists, if not, install it\n",
    "try:\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
    "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
    "except Exception as e:\n",
    "    print(\"Installing missing library\")\n",
    "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")"
   ],
   "id": "2dd514b59247a911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing missing library\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[95]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchmetrics\u001B[39;00m,\u001B[38;5;250m \u001B[39m\u001B[34;01mmlxtend\u001B[39;00m\n\u001B[32m      4\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmlxtend version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmlxtend.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torchmetrics'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[95]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mInstalling missing library\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m get_ipython().system(\u001B[33m\"\u001B[39m\u001B[33mpip install -q torchmetrics -U mlxtend # <- Note: If you\u001B[39m\u001B[33m'\u001B[39m\u001B[33mre using Google Colab, this may require restarting the runtime\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchmetrics\u001B[39;00m,\u001B[38;5;250m \u001B[39m\u001B[34;01mmlxtend\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmlxtend version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmlxtend.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torchmetrics'"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ],
   "id": "f3db0861391de3e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Setup confusion matrix instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
    "confmat_tensor = confmat(\n",
    "    preds=y_pred_tensor, \n",
    "    target=test_data.targets\n",
    ")\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n",
    "    class_names=class_names, # turn the row and column labels into class names\n",
    "    figsize=(10, 7)\n",
    ")"
   ],
   "id": "5fa2d0ba1c5dee61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#",
   "id": "db6e7027ff19e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "Let's finish this section off by saving and loading in our best performing model.\n",
    "\n",
    "We can save and load a PyTorch model using a combination of:\n",
    "- torch.save - a function to save a whole PyTorch model or a model's state_dict().\n",
    "- torch.load - a function to load in a saved PyTorch object.\n",
    "- torch.nn.Module.load_state_dict() - a function to load a saved state_dict() into an existing model instance.\n",
    "- \n",
    "You can see more of these three in the PyTorch saving and loading models documentation.\n",
    "For now, let's save our model_2's state_dict() then load it back in and evaluate it to make sure the save and load went correctly."
   ],
   "id": "45c4da9df416b434"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:19:44.068855Z",
     "start_time": "2025-04-24T12:19:44.059702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(\n",
    "    parents=True, # create parent directories if needed\n",
    "    exist_ok=True # if models directory already exists, don't error\n",
    ")\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ],
   "id": "cf809861d0754873",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/03_pytorch_computer_vision_model_2.pth\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e0754b4d1e84c15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now we've got a saved model state_dict() we can load it back in using a combination of load_state_dict() and torch.load().\n",
    "\n",
    "Since we're using load_state_dict(), we'll need to create a new instance of FashionMNISTModelV2() with the same input parameters as our saved model state_dict()."
   ],
   "id": "1557a143960629ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:21:20.199409Z",
     "start_time": "2025-04-24T12:21:20.184996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n",
    "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
    "loaded_model_2 = FashionMNISTModelV2(\n",
    "    input_shape=1, \n",
    "    hidden_units=10, # try changing this to 128 and seeing what happens \n",
    "    output_shape=10\n",
    ") \n",
    "\n",
    "# Load in the saved state_dict()\n",
    "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "\n",
    "# Send model to GPU\n",
    "loaded_model_2 = loaded_model_2.to(DEVICE)"
   ],
   "id": "7568823c275177f3",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:21:21.198074Z",
     "start_time": "2025-04-24T12:21:21.195770Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model_2\n",
   "id": "53a99c0505afd153",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV2(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:21:24.057241Z",
     "start_time": "2025-04-24T12:21:23.097731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate loaded model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loaded_model_2_results = eval_model(\n",
    "    model=loaded_model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, \n",
    "    accuracy_fn=accuracy_fn\n",
    ")"
   ],
   "id": "4c9c4d096e225ba8",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:21:24.238006Z",
     "start_time": "2025-04-24T12:21:24.235924Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model_2_results\n",
   "id": "535f66a19de54812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV2',\n",
       " 'model_loss': 2.3023064136505127,\n",
       " 'model_acc': 9.994009584664537}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f11e877907c770f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
